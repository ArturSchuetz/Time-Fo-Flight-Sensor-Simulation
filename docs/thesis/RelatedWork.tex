\documentclass[thesis.tex]{subfiles}
\begin{document}

\chapter{Verwandte Arbeiten}
\label{chap:relatedwork}

Dieses Kapitel beschäftigt sich mit Forschungsarbeiten, die ein ähnliches oder das selbe Ziel verfolgen. Deren Ergebnisse werden im Kontext der Zielsetzung dieser Arbeit bewertet.

\section{Photon Mapping based Simulation of Multi-Path Reflection Artifacts in Time-of-Flight Sensors}

Die Arbeit von Meister et al. \cite{bib:Meister2012PhotonMB} ist die erste Arbeit, die den Einfluss von indirekter Beleuchtung auf die Tiefendaten der Time-of-Flight Sensoren durch ein globales Beleuchtungsmodell simuliert. Dabei konzentriert sie sich auf die Simulation des durch indirekte Beleuchtung verursachten Multipath Fehlers, der in \autoref{sec:multipath_problem} genauer untersucht wird. Zu diesem Zweck wurde die \emph{Photon Mapping} Implementierung des LuxRenders modifiziert und die Beleuchtungsberechnung um eine zeitliche Komponente erweitert. Photon Mapping ist ein Verfahren, das 1996 von Jensen \cite{bib:Jensen96} vorgestellt wurde und die Rendergleichung löst, indem in zwei Phasen zunächst Photonen durch Lichtquellen in die Szene geschossen werden und diese anschließend in der zweiten Phase beim Berechnen des Bildes zur Beleuchtung verwendet werden.

In der ersten Phase werden von jeder Lichtquelle aus virtuelle Photonen emittiert. Beim der Kollision des Photons mit der Oberfläche der Geometrie werden dessen Position, Intensität und Einfallswinkel in einer Punktewolke gespeichert.  Meister et al. \cite{bib:Meister2012PhotonMB} erweitern den Datensatz zusätzlich um die zurückgelegte Distanz des Photons. Anschließend wird das Photon entweder entsprechend der Eigenschaften der definierten BRDF reflektiert oder vollständig absorbiert. Dies wird fortgeführt, bis eine vordefinierte Anzahl an Photonen in die Szene emittiert worden sind.

In der zweiten Phase wird die Szene mittels Ray Tracing gerendert. Dazu werden pro Pixel mehrere Strahlen aus der Richtung des Betrachters in die Szene geschossen. Bei jeder Kollision eines Strahls mit der Szene wird am Schnittpunkt die Strahlungsdichte in Richtung des Betrachters berechnet, indem für die direkte Beleuchtung für jede Lichtquelle getestet wird, ob diese vom Schnittpunkt aus sichtbar ist. Für die indirekte Beleuchtung werden die Photonen in einem Radius zur Berechnung der Beleuchtungsdichte herangezogen und die Reflexion wird in Richung des Betrachters entsprechend der BRDF der Oberfläche bestimmt. Die zurückgelegte Distanz des Photons ist sowohl für die direkte als auch für die indirekte Beleuchtung bekannt, was die Berechnung der einzelnen Phasenbilder betrifft.

Meister et al. \cite{bib:Meister2012PhotonMB} simulieren in ihrer Arbeit ausschließlich sinusförmige Signale, wie sie in \autoref{sec:modulated_sinus_signal} beschrieben sind. Der Algorithmus läuft komplett auf der CPU und braucht für die Generierung eines 200x200 Tiefenbildes laut eigenen Angaben mit einem 2,4 GHz Xeon Prozessor ungefähr 2 Stunden. Dazu wird jedes der vier Phasenbilder, welche die Ladungen $Q_1$, $Q_2$, $Q_3$ und $Q_4$ enthalten, nacheinander generiert und das Tiefenbild anschließend mittels \autoref{eq:sin_phase_calculation} berechnet.

Die Ergebnisse der Simulation zeigen, dass sich Fehler, die durch Interreflexionen verursacht werden, mittels Photon Mapping simulieren lassen. Allerdings konnten nicht alle beobachteten Artefakte korrekt simuliert werden, die im Testaufbau beobachtet wurden, die besonders durch flache Einfallswinkel des Lichts auf die Oberfläche verursacht werden. Die hohe Berechnungsdauer schränkt den Einsatz zusätzlich auf statische Szenen ein, da die Berechnung dynamischer Szenen mit einem Umfang weniger Sekunden mehrere Tage oder Wochen benötigen würde.

\section{Simulation of Time-of-Flight Sensors using Global Illumination}

Die Arbeit von Meister et al. \cite{bib:Meister2013} kommt dem Ansatz dieser Arbeit am nächsten. Sie baut auf den Erfahrungen ihrer vorangegangenen Arbeit \cite{bib:Meister2012PhotonMB} auf und konzentriert sich darauf Multiple Path Reflexionen mittels bidirektionalem Path Tracing zu simulieren. Die Arbeit konzentriert sich dabei ebenfalls auf die Simulation des Multipath Problems und erweitert dafür den Path Tracing Algorithmus des LuxRender um eine zeitliche Komponente. Das Grundprinzip eines Path Tracing Algorithmus wurde bereits in \autoref{sec:path_tracing} beschrieben und wird in \autoref{chap:simulation} im Detail erläutert. Bei bidirektionalem Path Tracing handelt es sich um eine Erweiterung dieses Prinzips. Dabei werden zusätzlich zu den Strahlen aus der Position des Betrachters, Strahlen von der Lichtquelle in die Szene geschossen und die Schnittpunkte der Strahlen mit der Geometrie jeweils verbunden. Durch diese Optimierung konvergieren besonders Szenen, die ausschließlich indirekt beleuchtet werden, deutlich schneller zum Ergebnis.

Meister et al. \cite{bib:Meister2013} erweiterten diesen bidirektionalen Path Tracer um eine zeitliche Komponente, indem jedem Sicht- und Lichtstrahl bei der Berechnung eines Schnittpunktes mit der Geometrie die zurückgelegte Strecke mitgegeben wurde. Anschließend wird jeder Lichtbeitrag zum Ergebnis gewichtet aufsummiert. Jedes Phasenbild wird dabei wiederum unabhängig von den anderen gerendert. Aus allen wird anschließend das Tiefenbild berechnet, während bei der Generierung der Phasenbilder ausschließlich sinusförmige Signale betrachtet werden.

Die Ergebnisse der Simulation variierten in Abhängigkeit zu der gewählten maximalen Tiefe der verfolgten Pfade. So gab es bei einer Pfadlänge von 1 keinerlei Interreflexionen und die Tiefenwerte entsprachen dem Ground Truth Datensatz, während mit jeder weiteren Erhöhung der Pfadlänge ein stärker Einfluss durch Interreflexionen gemessen werden konnte. Allerdings traten ebenfalls Artefakte auf, die durch den Path Tracer verursacht wurden. Diese Artefakte verringerten sich bei höheren Pfadlängen und ab einer Pfadlänge von 8 waren keine Unterschiede zu höheren Pfadlängen zu erkennen. Allerdings konnten Effekte durch starke Interreflexionen an Kanten, die sich in \glq Ausbeulungen\grq{} im Tiefenbild äußern, durch dieses Verfahren nicht simuliert werden. 

Meister et al. \cite{bib:Meister2013} sind zu dem Schluss gekommen, dass Photon Mapping weniger als Path Tracing Ansätze dazu geeignet ist, um Time-of-Flight Sensoren zu simulieren. Durch Modifikation des bidirektionalen Path Tracing Algorithmus konnten die Auswirkungen indirekter Beleuchtung auf das Tiefenbild physikalisch plausibel simuliert werden, auch wenn nicht alle Probleme adressiert werden konnten. So wurden ausschließlich ideale sinusförmige Wellen angenommen und die Auswirkungen der Non-Linearität der Sensoren nicht berücksichtigt. Allerdings wird der Algorithmus auf der CPU ausgeführt, was eine hohe Berechnungsdauer mit sich zieht und daher nicht für Simulationen mit einer hohen Anzahl an Bildern geeignet ist.

\section{Detailed Modelling and Calibration of a Time-of-Flight Camera}

Die Arbeit von Hertzberg und Frese \cite{bib:Hertzberg2014} konzentriert sich auf eine detaillierte physikalisch motivierte Kalibrierung der PMD CamBoard Kamera zur Korrektur von typischen Artefakten, die durch Time-of-Flight Kameras erzeugt werden, die mit sinusförmig moduliertem Licht arbeiten. Hertzberg und Frese \cite{bib:Hertzberg2014} führen dazu ein idealistisches Sensor Modell ein und ergänzen dieses Modell um typische Abweichungen, die in der Realität existieren. Dabei konzentriert sich die Arbeit auf die detaillierte Kalibrierung des Sensors und ignoriert Effekte, die durch die Lichtausbreitung verursacht werden. Zu diesem Zweck wird die erwähnte Kamera mittels eines Schachbrettes kalibriert, um die Linsenverzeichnung zu ermitteln. Zusätzlich wird die Abweichung und die Non-Linearität einzelner Pixel korrigiert und die Vignettierung der Infrarot LED berücksichtigt.

Ein besonderer Fokus liegt auf der Korrektur des \emph{Lens Scattering} Effekts, der in \autoref{sec:lens_scattering} im Detail untersucht wird. Dazu wird ein Retroreflektor vor einen wenig reflektierenden Hintergrund positioniert und der Einfluss gemessen, den das retroreflektierte Licht auf die umliegenden Pixel hat. Aus den Ergebnissen der Messung wurde eine Punktspreizfunktion berechnet, die sich aus einer Summe von Gaußfunktionen zusammensetzt und den Einfluss eines Pixels auf seine Nachbarpixel angibt. Diese Funktion wird genutzt um dein Einfluss des Lens Scattering zu kompensieren.

Abschließend wurde gezeigt, wie mit Hilfe von 64 Einzelbildern einer statischen Szene mit unterschiedlichen Belichtungszeiten sowohl ein HDR, als auch ein Tiefenbild erzeugt werden konnte. Dabei wurden allerdings Effekte durch Interreflexionen des Lichts ignoriert, beispielsweise der Einfluss von Umgebungslicht und der Einfluss der Temperatur auf den Sensor.

\section{Real-time Simulation of Time-of-Flight Sensors and Accumulation of Range Camera Data}

Das Kapitel 3 der Dissertation von Keller \cite{bib:Keller2015} beschäftigt sich mit der Echtzeit Simulation von Time-of-Flight Kameras und typischen Artefakten wie \emph{Bewegungsunschärfe}, die verursacht werden, wenn sich Objekte während der Aufnahme im Bild bewegen, \emph{Mixed Pixels}, die Thema in \autoref{sec:mixed_pixels} sind, und systematische Abweichungen, die in \autoref{sec:systematoc_error} untersucht werden. Das Hauptziel der Simulation von Keller ist die Generierung von synthetischen Tiefensensordaten und die Möglichkeit Kameraparameter zur Laufzeit zu manipulieren. 

Zur Simulation des Tiefensensors wird die Rasterisierungspipeline der GPU verwendet, indem für OpenGL Vertex und Pixelschader entwickelt wurden, die Phasenbilder generieren. Dazu wird die Tiefeninformation, die die OpenGL Render Pipeline bereits liefert, genutzt, um die Phasenverschiebung der reflektierten Wellenfunktion zu bestimmen. Da jeder Pixel nur einen Sichtstrahl simulieren würde, wird zur Simulation des Mixed Pixels Fehlers das Bild in doppelter Auflösung gerendert und jeder Pixel des Endbildes aus dem Durchschnitt von vier Pixeln des Bildes in höherer Auflösung gebildet. Die Bewegungsunschärfe wird simuliert, indem das Bild vier Mal in unterschiedlichen Zeitschritten gerendert wird und anschließend die Phasenbilder aus dem Durchschnitt aller vier Einzelbilder berechnet werden.

Der systematische Fehler wird simuliert, indem dem die Abweichung von Ground Truth Tiefenwerten mit einem echten Sensor gemessen und diese in einer Textur gespeichert werden. Diese Textur dient im Anschluss als Look-up Table, um den tiefenwertabhängigen systematischen Fehler auf das Tiefenbild aufzurechnen. Da die Messungen der echten Tiefenwerte durch ein statisches Rauschen beeinflusst werden, wird die Fehlerfunktion mit Hilfe eines Sinussignals approximiert, das durch die Fourier-Transformation der Messwerte berechnet wird.

Der zufällige Fehler wird für jedes Phasenbild einzeln berechnet, indem intensitätsabhängiges Rauschverhalten angenommen wird und durch eine Gaußverteilung approximiert wird. Die Intensität des zufälligen Fehlers wird durch zwei Koeffizienten $\alpha, \beta \geq 0$ gesteuert und das Phasenbild wird folgendermaßen modifiziert:
\begin{equation}
A_{noisy,i} = A_i + \alpha \cdot n_{rand} + A_i \cdot \beta \cdot n_{rand},
\end{equation}
wobei $A_i$ für ein Phasenbild steht und $n_{rand} \in [-1,1]$ eine normalverteilte Zufallszahl darstellt.

Die in \cite{bib:Keller2015} vorgestellte Simulation zeigt hohe Übereinstimmungen mit dem zur Kalibrierung genutzten Sensor in Bezug auf den systematischen Fehler und das Mixed Pixels Verhalten, basiert allerdings nicht auf einer physikalischen Simulation, sondern auf der Nachbildung des eingemessenen Verhaltens des Kamerasystems. Dabei werden Fehler, die durch die Lichtausbreitung verursacht werden wie Fehler durch Lens Scattering oder Interreflexionen nicht adressiert.

\section{Zusammenfassung}

Nur wenige Simulationen erfüllen die Zielsetzung dieser Arbeit. Vor allem zu erwähnen sind dabei die Arbeiten von Meister et al. \cite{bib:Meister2012PhotonMB}\cite{bib:Meister2013}, die durch die Simulation der Lichtausbreitung erstmals Artefakte simulieren konnten, die durch Interreflexionen erzeugt werden. Allerdings werden dazu Beleuchtungsmodelle verwendet, die stark von gemessenen Daten abweichen, weshalb nicht alle durch Interreflexionen verursachten Artefakte simuliert werden konnten. Darüber hinaus wurde angenommen, dass sich die Lichtquelle und die Kamerablende an derselben Position befinden, was von der Realität abweicht. 

Häufig wird angemerkt, dass aufgrund der Komplexität der Berechnung der Einsatz in interaktiven Anwendungsgebieten nicht geeignet ist \cite{bib:Hertzberg2014}\cite{bib:Lambers2015}. Keller \cite{bib:Keller2015} und Lamberts et al. \cite{bib:Lambers2015} stellten in ihren Arbeiten daher Simulationen vor, die auf der GPU ausgeführt werden können und mit Hilfe der Rendering Pipeline durch Verwendung von Vertex- und Pixelshadern entwickelt wurden. Durch Verwenduung der OpenGL Rasterisierungspipeline ergeben sich Einschränkungen, weshalb die Arbeiten auf physikalisch motivierte Simulationen verzichten und stattdessen das Tiefenbild oder die einzelnen Phasenbilder nach zuvor eingemessenen Abweichungen echter Time-of-Flight Kamerasysteme modifizieren.

\subfilebib % Makes bibliography available when compiling as subfile
\end{document}

